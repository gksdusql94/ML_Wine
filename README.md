# ðŸ· Wine Quality Prediction Using KNN and Logistic Regression

This project focuses on predicting wine quality based on various chemical properties using machine learning models. We implemented two models: K-Nearest Neighbors (KNN) and Logistic Regression, comparing their performance and optimizing hyperparameters to achieve the best results.

## ðŸŽ¯ Objective

The objective of this project is to build and evaluate machine learning models that can accurately predict wine quality (good or bad) based on chemical features such as acidity, sugar content, pH, and alcohol levels.

## ðŸ“Š Dataset

The dataset used in this project is a real-world dataset of wine quality, consisting of over 4,000 entries and 11 features, including:

- Fixed acidity
- Volatile acidity
- Citric acid
- Residual sugar
- Chlorides
- Free sulfur dioxide
- Total sulfur dioxide
- Density
- pH
- Sulphates
- Alcohol

The target variable is wine quality, which is classified into two categories:
- **Good quality**: wine quality > 5
- **Bad quality**: wine quality â‰¤ 5

## ðŸ”‘  Key Achievements

1. **Data Preprocessing**: Cleaned and standardized the dataset for better model performance. Removed redundant features based on correlation analysis.
2. **Model Implementation**:
   - Developed custom KNN and Logistic Regression models.
   - Experimented with different hyperparameters such as distance metrics (Euclidean/Manhattan) and regularization types (L1/L2).
3. **Model Evaluation**: 
   - Used cross-validation to evaluate the models.
   - Achieved the best F1 score of 0.87 with the KNN model using Manhattan distance and inverse distance weighting.
     
4. **Performance Improvement**:
   - Standardized the dataset to enhance model accuracy.
   - The KNN model outperformed Logistic Regression, with a performance improvement of 10% after standardization.

## ðŸ§  Results

The KNN model with Manhattan distance and inverse distance weighting proved to be the most effective, achieving an F1 score of 0.87. Standardizing the data significantly improved model performance, especially for KNN, which outperformed Logistic Regression on this dataset.

- Developed a home price prediction model using Kaggleâ€™s Ames Housing Dataset (2,919 records, 79 features), utilizing K-Nearest Neighbors (KNN) to estimate new house prices.
- Performed statistical analysis and correlation analysis on 5 variables to decide their impact on home prices.
-	Feature Engineering: Created critical predictive variables to enhance model performance.
-	Applied the KNN algorithm and predicted the price of a new house based on the average price of the five most similar houses, arriving at an estimated price of $121,080.


| Model                    | F1 Score | Precision | Recall | Accuracy |
|--------------------------|----------|-----------|--------|----------|
| KNN (Manhattan, IDW)      | 0.87     | 0.86      | 0.89   | 0.83     |
| Logistic Regression (L2)  | 0.82     | 0.81      | 0.84   | 0.79     |

## âš™ï¸ Skills & Tools

- **Languages/Tools**: Python, Pandas, NumPy, Scikit-learn
- **Data Visualization**: Matplotlib, Seaborn
- **Model Evaluation**: Precision-recall analysis, ROC-AUC curves, cross-validation

## ðŸ’» How to Run
Clone the repository:
```bash
git clone https://github.com/gksdusql94/ML_Wine.git
```


## ðŸ“ˆVisuals

### Pair Plot of Features vs. Quality

```python
import seaborn as sns
sns.pairplot(df, hue = 'quality') # I want to compare all files with quality
plt.show()
```
![image](https://github.com/user-attachments/assets/c9072b96-6bca-40f4-a3cc-ca9a87769a59)


### ROC Curve for KNN Model

```python
    # Plot ROC curve
    plt.figure()
    plt.plot(fpr, tpr, label='ROC curve (area = %0.2f)' % roc_auc)
    plt.plot([0, 1], [0, 1], 'k--')
    plt.xlim([0.0, 1.0])
    plt.ylim([0.0, 1.05])
    plt.xlabel('False Positive Rate')
    plt.ylabel('True Positive Rate')
    plt.title('ROC Curve for KNN')
    plt.legend(loc="lower right")
    plt.show()

    # Find optimal threshold
    f1_scores = [f1_score(test_labels, [1 if x >= t else 0 for x in knn_probs]) for t in thresholds]
    optimal_idx = np.argmax(f1_scores)
    optimal_threshold = thresholds[optimal_idx]
    print("Optimal Threshold for KNN:", optimal_threshold)

    # Predict labels using the optimal threshold
    optimal_pred = [1 if x >= optimal_threshold else 0 for x in knn_probs]

    # Print F1 score using the optimal threshold
    print("F1 Score using Optimal Threshold:", f1_score(test_labels, optimal_pred))

except Exception as e:
    print("Error:", e)
```
![image](https://github.com/user-attachments/assets/1e437d29-05fb-4a11-a25c-abb69439f7d0)


